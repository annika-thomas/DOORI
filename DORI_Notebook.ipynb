{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fb3c91-dc04-4cc8-8d25-53eb02723f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FastSAM.fastsam import *\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from utils import compute_blob_mean_and_covariance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plotErrorEllipse\n",
    "import skimage\n",
    "from ultralytics import SAM\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63848cc9-a660-423f-93ce-cc553fdcd150",
   "metadata": {},
   "source": [
    "- Load FastSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c80163-9cc0-44f7-ae58-9e1a513a426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to checkpoint. (If checkpoint does not exist, the implementation in FastSAM repo downloads it.)\n",
    "fastSamModel = FastSAM('./FastSAM/Models/FastSAM-x.pt')\n",
    "# fastSamModel = SAM('sam_b.pt')\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ae2a6-5ca3-49af-9442-98418af8dff4",
   "metadata": {},
   "source": [
    "- put your filename here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08dead5f-e8a1-4a43-80de-cca3758c8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file location\n",
    "# file_location = 'fish_img'\n",
    "file_location = 'fish_img_1'\n",
    "\n",
    "# Get all image file names in the directory\n",
    "image_files = [f for f in os.listdir(file_location) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Setup output folder\n",
    "outputFolder = 'output_test/'\n",
    "\n",
    "# Check if output folder exists and create it if necessary\n",
    "if (not os.path.isdir(outputFolder)):\n",
    "    os.makedirs(outputFolder)\n",
    "\n",
    "# Sort the image files by their filenames such that frame0 then frame1 then frame2 ... then frame 10 then 11 then 12, etc\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0].split('e')[-1]))\n",
    "\n",
    "# For demonstration purpose, we only use 850~900\n",
    "image_files = image_files[850:950]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c932c3-00ab-4cdd-ad1c-11b013a675af",
   "metadata": {},
   "source": [
    "- DORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958d5f2c-db95-4a21-89a6-8d29882dace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 1 object, 466.2ms\n",
      "Speed: 2.9ms preprocess, 466.2ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 900 0.49759554862976074\n",
      "crab iter: 900 0.33682695031166077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 426.6ms\n",
      "Speed: 1.5ms preprocess, 426.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 901 0.4980039596557617\n",
      "crab iter: 901 0.34223178029060364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 425.6ms\n",
      "Speed: 1.7ms preprocess, 425.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 902 0.4966365098953247\n",
      "crab iter: 902 0.3425745964050293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 426.7ms\n",
      "Speed: 1.5ms preprocess, 426.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 903 0.5044446587562561\n",
      "crab iter: 903 0.35114261507987976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 434.8ms\n",
      "Speed: 2.7ms preprocess, 434.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 904 0.45147180557250977\n",
      "crab iter: 904 0.2864118814468384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 433.2ms\n",
      "Speed: 2.6ms preprocess, 433.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 905 0.44639888405799866\n",
      "crab iter: 905 0.28739604353904724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 427.4ms\n",
      "Speed: 1.6ms preprocess, 427.4ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 906 0.4621506929397583\n",
      "crab iter: 906 0.27979740500450134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 436.0ms\n",
      "Speed: 1.5ms preprocess, 436.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 921 0.37873613834381104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x1024 1 object, 435.1ms\n",
      "Speed: 2.7ms preprocess, 435.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish iter: 922 0.3959563970565796\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the average luminance of each superpixel\n",
    "average_luminance = []\n",
    "\n",
    "# initialize count\n",
    "count = 0\n",
    "\n",
    "potential_fish_frames = []\n",
    "\n",
    "# For FastSam:\n",
    "# Specify confidence and IoU parameters (see FastSAM paper or rather YOLO v8 documentation)\n",
    "conf = 0.5\n",
    "iou = 0.9\n",
    "\n",
    "# index=0\n",
    "Y_OUT = np.zeros((1,len(image_files)+10))\n",
    "\n",
    "SAM_TIME = 0\n",
    "\n",
    "FIRST_WRITE_TXT = True\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "\n",
    "    START_TIME = time.time()\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(os.path.join(file_location, image_file))\n",
    "    \n",
    "    # Motion Detection (1):\n",
    "\n",
    "    luminance = np.mean(image)\n",
    "    count += 1\n",
    "\n",
    "    if count < 50:\n",
    "        average_luminance.append(luminance)\n",
    "        continue\n",
    "    \n",
    "    # # Append the average luminance to the list\n",
    "    average_luminance.append(luminance)\n",
    "\n",
    "    luminance_diff = abs(luminance - np.mean(average_luminance[count-50:count]))\n",
    "\n",
    "    if luminance_diff > 1:\n",
    "        \n",
    "        potential_fish_frames.append(count)\n",
    "\n",
    "        # Run FastSAM\n",
    "        everything_results = fastSamModel(image, device=DEVICE, retina_masks=True, imgsz=1024, conf=conf, iou=iou,)\n",
    "        prompt_process = FastSAMPrompt(image, everything_results, device=DEVICE)\n",
    "        segmask = prompt_process.everything_prompt()\n",
    "        # print(len(segmask))\n",
    "\n",
    "        # Segment the images for better classification\n",
    "        image_array = []\n",
    "        for mask_index in range(len(segmask)):\n",
    "            # Masked out the segmentation\n",
    "            potential_fish_img = image.copy()\n",
    "            segmask = segmask.type(torch.int32)\n",
    "            potential_fish_img[~segmask[mask_index,:,:],:] = 0\n",
    "\n",
    "            # Convert to PIL Image\n",
    "            image_pil = Image.fromarray(potential_fish_img)\n",
    "            image_array.append(image_pil)\n",
    "\n",
    "        # Initialize the CLIP model and processor\n",
    "        model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        \n",
    "        # Classification labels\n",
    "        # classes = ['fish', 'aquatic plants', 'trash', 'water', 'land', 'ocean', 'submarine']\n",
    "        classes = ['fish', 'aquatic plants', 'trash', 'water', 'land', 'ocean', 'submarine']\n",
    "        \n",
    "        # Process the image for CLIP (resize, convert to tensor, normalize)\n",
    "        inputs = processor(text=classes, images=image_array, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Forward pass: get the image features from CLIP\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Classification:\n",
    "        logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) \n",
    "        \n",
    "        classification_outputs = torch.argmax(probs, dim=1)\n",
    "        Y_OUT[:,count] = probs[:,0].max().item()\n",
    "\n",
    "        END_TIME = time.time()\n",
    "\n",
    "        # If is classified as 'crab'\n",
    "        if torch.any(classification_outputs==0):\n",
    "            # Save the image that is detected and classified with fish in the output Folder\n",
    "            # cv2.imwrite(outputFolder+\"crab_\"+image_file, image) \n",
    "            if FIRST_WRITE_TXT:\n",
    "                print(\"fish iter: \" + str(count+850) + \" \" + str(probs[:,0].max().item()) )\n",
    "                f = open(\"{outputFolder}/fish_output.txt\".format(outputFolder=outputFolder), \"w\")\n",
    "                f.write(str(count) + \" \" + str(probs[:,0].max().item()))\n",
    "                f.write('\\n')\n",
    "                f.close()\n",
    "                FIRST_WRITE_TXT = False\n",
    "            else:\n",
    "                print(\"fish iter: \" + str(count+850) + \" \" + str(probs[:,0].max().item()) )\n",
    "                f = open(\"{outputFolder}/fish_output.txt\".format(outputFolder=outputFolder), \"a\")\n",
    "                f.write(str(count) + \" \" + str(probs[:,0].max().item()))\n",
    "                f.write('\\n')\n",
    "                f.close()\n",
    "\n",
    "        classes = ['crab', 'aquatic plants', 'trash', 'water', 'land', 'ocean', 'submarine']\n",
    "        \n",
    "        # Process the image for CLIP (resize, convert to tensor, normalize)\n",
    "        inputs = processor(text=classes, images=image_array, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Forward pass: get the image features from CLIP\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Classification:\n",
    "        logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) \n",
    "        \n",
    "        classification_outputs = torch.argmax(probs, dim=1)\n",
    "        Y_OUT[:,count] = probs[:,0].max().item()\n",
    "\n",
    "        # If is classified as 'crab'\n",
    "        if torch.any(classification_outputs==0):\n",
    "            # Save the image that is detected and classified with fish in the output Folder\n",
    "            # cv2.imwrite(outputFolder+\"crab_\"+image_file, image) \n",
    "            if FIRST_WRITE_TXT:\n",
    "                print(\"crab iter: \" + str(count+850) + \" \" + str(probs[:,0].max().item()) )\n",
    "                f = open(\"{outputFolder}/crab_output.txt\".format(outputFolder=outputFolder), \"w\")\n",
    "                f.write(str(count) + \" \" + str(probs[:,0].max().item()))\n",
    "                f.write('\\n')\n",
    "                f.close()\n",
    "                FIRST_WRITE_TXT = False\n",
    "            else:\n",
    "                print(\"crab iter: \" + str(count+850) + \" \" + str(probs[:,0].max().item()) )\n",
    "                f = open(\"{outputFolder}/crab_output.txt\".format(outputFolder=outputFolder), \"a\")\n",
    "                f.write(str(count) + \" \" + str(probs[:,0].max().item()))\n",
    "                f.write('\\n')\n",
    "                f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44c1ae-d9e6-4146-bdf0-c7d5494954da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
